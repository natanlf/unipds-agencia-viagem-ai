# Configura a URL base do servidor Ollama.
quarkus.langchain4j.ollama.base-url=http://localhost:11434/

# Define o ID do modelo que o Langchain4j ir? invocar no Ollama.
quarkus.langchain4j.ollama.chat-model.model-id=gpt-oss:20b

# Define um timeout para a chamada.
# Se o modelo demorar mais de 60s para responder, a chamada falhar?.
quarkus.langchain4j.ollama.timeout=60s

# --- Configura??es do EasyRAG (desativadas) ---
# quarkus.langchain4j.easy-rag.path=src/main/resources/rag
# quarkus.langchain4j.easy-rag.max-segment-size=100
# quarkus.langchain4j.easy-rag.max-overlap-size=20

# --- Configura??o do Embedding Model ---
quarkus.langchain4j.ollama.embedding-model.model-id=nomic-embed-text

# --- Configura??o do PgVector ---
# A dimens?o do vetor DEVE corresponder ? do seu EmbeddingModel.
# O modelo 'nomic-embed-text' usa vetores de 768 dimens?es.
quarkus.langchain4j.pgvector.register-vector-pg-extension=true
quarkus.langchain4j.pgvector.dimension=768
quarkus.langchain4j.pgvector.table=travel_embeddings
quarkus.langchain4j.pgvector.drop-table-first=true

# --- Configura??o de Logging do LangChain4j ---
# Loga a requisi??o completa enviada para o LLM (incluindo o manifesto de Tools)
quarkus.langchain4j.log-requests=true

# Loga a resposta completa recebida do LLM (incluindo a chamada de Tool)
quarkus.langchain4j.log-responses=true